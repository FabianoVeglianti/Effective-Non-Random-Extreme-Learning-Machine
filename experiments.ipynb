{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\fabia\\OneDrive\\Desktop\\ELM_Simulazioni\\.venv\\lib\\site-packages\\tf2jax\\_src\\tf2jax.py:305: The name tf.NodeDef is deprecated. Please use tf.compat.v1.NodeDef instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from elm import ELM, approximated_ENRELM, incremental_ENRELM\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import logger\n",
    "import datagenerator\n",
    "import dataloader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looped_ELM(X_train, X_test, y_train, y_test, loops = 20):\n",
    "    hidden_space_dim = int(np.minimum(50 * X_train.shape[0], round(X_train.shape[1] * 0.5)))\n",
    "    \n",
    "    test = {\n",
    "        'X': X_test,\n",
    "        'y': y_test\n",
    "    }\n",
    "\n",
    "    loops = 20\n",
    "    loops_train_err_ELM = np.zeros(shape=(loops,hidden_space_dim))\n",
    "    loops_test_err_ELM = np.zeros(shape=(loops,hidden_space_dim))\n",
    "\n",
    "    total_timing = 0                                                                            \n",
    "\n",
    "    for loop in range(loops):\n",
    "        for n in range(1,hidden_space_dim+1):\n",
    "            _, _, results = ELM(n, -1, X_train, y_train, test)\n",
    "            loops_train_err_ELM[loop, n-1] = results['training_error']\n",
    "            loops_test_err_ELM[loop, n-1] = results['test_error']\n",
    "            total_timing += results['timing']\n",
    "\n",
    "                                                                          \n",
    "\n",
    "    mean_loop_timing = total_timing / loops\n",
    "\n",
    "    training_error = np.mean(loops_train_err_ELM, axis = 0)\n",
    "    std_training_error = np.std(loops_train_err_ELM, axis = 0)\n",
    "    min_training_error = np.min(loops_train_err_ELM, axis = 0)\n",
    "    max_training_error = np.max(loops_train_err_ELM, axis = 0)\n",
    "\n",
    "    test_error = np.mean(loops_test_err_ELM, axis = 0)\n",
    "    std_test_error = np.std(loops_test_err_ELM, axis = 0)\n",
    "    min_test_error = np.min(loops_test_err_ELM, axis = 0)\n",
    "    max_test_error = np.max(loops_test_err_ELM, axis = 0)\n",
    "\n",
    "\n",
    "    data = {\n",
    "        'training_error': training_error,\n",
    "        'loops_training_error': loops_train_err_ELM,\n",
    "        'std_training_error': std_training_error,\n",
    "        'min_training_error': min_training_error,\n",
    "        'max_training_error': max_training_error,\n",
    "        'test_error': test_error,\n",
    "        'loops_test_error': loops_test_err_ELM,\n",
    "        'std_test_error': std_test_error,\n",
    "        'min_test_error': min_test_error,\n",
    "        'max_test_error': max_test_error,\n",
    "        'total_timing': total_timing,\n",
    "        'mean_loop_timing': mean_loop_timing\n",
    "        }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def single_dataset_simulation(X,y):\n",
    "    seed_train_test_split = 1234\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.T ,y, random_state = seed_train_test_split)\n",
    "    X_train, X_test, y_train, y_test, mean_y_train = utils.preprocess(X_train, X_test, y_train, y_test)\n",
    "    X_train = X_train.T\n",
    "    X_test = X_test.T\n",
    "\n",
    "    \n",
    "    test = {\n",
    "        'X': X_test,\n",
    "        'y': y_test\n",
    "    }\n",
    "\n",
    "    # approximated ENR-ELM\n",
    "    _, _, results_approximated_ENRELM = approximated_ENRELM(X_train, y_train, sort_by_correlation=True, test=test)  \n",
    "\n",
    "    # incremental ENR-ELM\n",
    "    _, _, _, results_incremental_ENRELM = incremental_ENRELM(X_train, y_train, epsilon = 1/np.sqrt(y_train.shape[0]), threshold=1e-5, test = test)\n",
    "\n",
    "    #ELM\n",
    "    results_ELM = looped_ELM(X_train, X_test, y_train, y_test, loops = 20)\n",
    "    \n",
    "    results = [results_approximated_ENRELM, results_incremental_ENRELM, results_ELM]\n",
    "    suffixes = [\"_approximated_ENRELM\", \"_incremental_ENRELM\", \"_ELM\"]\n",
    "\n",
    "    data = {}\n",
    "    for result, suffix in zip(results, suffixes):\n",
    "        for key, value in result.items():\n",
    "            data[key + suffix] = value\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_csv(filename, df_times):\n",
    "    # Check if the file exists\n",
    "    folder_path = 'results/datasets'\n",
    "    full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    df_times.to_csv(full_path, sep=\";\", index=False)\n",
    "    #if not os.path.isfile(filename):\n",
    "        # File does not exist, save df_times as is\n",
    "\n",
    "def write_np_npz(filename, data):\n",
    "    folder_path = 'results/datasets'\n",
    "    full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    # Create the directories if they don't exist\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    np.savez_compressed(full_path, **data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abalone\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Auto MPG\n",
      "Bank\n",
      "California Housing\n",
      "Delta Ailerons\n",
      "LA Ozone\n",
      "Machine CPU\n",
      "Prostate Cancer\n",
      "Servo\n"
     ]
    }
   ],
   "source": [
    "loaders = {\n",
    "    1: (\"Abalone\", dataloader.load_abalone),\n",
    "    2: (\"Auto MPG\", dataloader.load_auto_mpg),\n",
    "    3: (\"Bank\", dataloader.load_bank),\n",
    "    4: (\"California Housing\", dataloader.load_california_housing),\n",
    "    5: (\"Delta Ailerons\", dataloader.load_delta_ailerons),\n",
    "    6: (\"LA Ozone\", dataloader.load_LAozone),\n",
    "    7: (\"Machine CPU\", dataloader.load_machine_cpu),\n",
    "    8: (\"Prostate Cancer\", dataloader.load_prostate),\n",
    "    9: (\"Servo\", dataloader.load_servo)\n",
    "}\n",
    "\n",
    "for key, value in loaders.items():\n",
    "    name = value[0]\n",
    "    loader = value[1]\n",
    "    generator = datagenerator.LoaderDataGenerator(name, loader)\n",
    "    res = generator.generate()\n",
    "    print(res['name'])\n",
    "    X = (res['data'][0]).T\n",
    "    y = res['data'][1].T\n",
    "\n",
    "    results = single_dataset_simulation(X, y)\n",
    "    write_np_npz(name + \"_results\" + \".npz\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abalone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_38332\\3188777625.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_times = pd.concat([df_times, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Auto MPG\n",
      "Bank\n",
      "California Housing\n",
      "Delta Ailerons\n",
      "LA Ozone\n",
      "Machine CPU\n",
      "Prostate Cancer\n",
      "Servo\n"
     ]
    }
   ],
   "source": [
    "iterations = 5\n",
    "df_times = pd.DataFrame(columns=[\"dataset\", \"iteration\", \"timing_approximated_ENRELM\", \"timing_incremental_ENRELM\", \"total_timing_ELM\", \"mean_loop_timing_ELM\"])\n",
    "loaders = {\n",
    "    1: (\"Abalone\", dataloader.load_abalone),\n",
    "    2: (\"Auto MPG\", dataloader.load_auto_mpg),\n",
    "    3: (\"Bank\", dataloader.load_bank),\n",
    "    4: (\"California Housing\", dataloader.load_california_housing),\n",
    "    5: (\"Delta Ailerons\", dataloader.load_delta_ailerons),\n",
    "    6: (\"LA Ozone\", dataloader.load_LAozone),\n",
    "    7: (\"Machine CPU\", dataloader.load_machine_cpu),\n",
    "    8: (\"Prostate Cancer\", dataloader.load_prostate),\n",
    "    9: (\"Servo\", dataloader.load_servo)\n",
    "}\n",
    "\n",
    "dataset_index = 0\n",
    "for key, value in loaders.items():\n",
    "    dataset_index += 1\n",
    "    name = value[0]\n",
    "    loader = value[1]\n",
    "    generator = datagenerator.LoaderDataGenerator(name, loader)\n",
    "    res = generator.generate()\n",
    "    X = res['data'][0]\n",
    "    y = res['data'][1]\n",
    "    print(name)\n",
    "\n",
    "    # Loop over iterations\n",
    "    for iteration in range(iterations):\n",
    "        results = single_dataset_simulation(X, y)\n",
    "        # Append a new row to the dataframe with the results and dataset name\n",
    "        df_times = pd.concat([df_times, pd.DataFrame({\n",
    "            \"dataset\": [name],\n",
    "            \"iteration\": [iteration],\n",
    "            \"timing_approximated_ENRELM\": [results[\"timing_approximated_ENRELM\"]],\n",
    "            \"timing_incremental_ENRELM\": [results[\"timing_incremental_ENRELM\"]],\n",
    "            \"total_timing_ELM\": [results[\"total_timing_ELM\"]],\n",
    "            \"mean_loop_timing_ELM\": [results[\"mean_loop_timing_ELM\"]]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "    # saving results when computing times is disabled\n",
    "    #write_np_npz(name + \"_results\" + \".npz\", results)\n",
    "\n",
    "# Save times\n",
    "write_df_csv(\"times_real.csv\", df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_1.csv\n",
      "dataset_2.csv\n",
      "dataset_3.csv\n",
      "dataset_4.csv\n",
      "dataset_5.csv\n",
      "dataset_6.csv\n",
      "dataset_7.csv\n",
      "dataset_8.csv\n",
      "dataset_9.csv\n",
      "dataset_10.csv\n",
      "dataset_11.csv\n",
      "dataset_12.csv\n",
      "dataset_13.csv\n",
      "dataset_14.csv\n",
      "dataset_15.csv\n",
      "dataset_16.csv\n",
      "dataset_17.csv\n",
      "dataset_18.csv\n",
      "dataset_19.csv\n",
      "dataset_20.csv\n",
      "dataset_21.csv\n",
      "dataset_22.csv\n",
      "dataset_23.csv\n",
      "dataset_24.csv\n",
      "dataset_25.csv\n",
      "dataset_26.csv\n",
      "dataset_27.csv\n",
      "dataset_28.csv\n",
      "dataset_29.csv\n",
      "dataset_30.csv\n",
      "dataset_31.csv\n",
      "dataset_32.csv\n",
      "dataset_33.csv\n",
      "dataset_34.csv\n",
      "dataset_35.csv\n",
      "dataset_36.csv\n",
      "dataset_37.csv\n",
      "dataset_38.csv\n",
      "dataset_39.csv\n",
      "dataset_40.csv\n",
      "dataset_41.csv\n",
      "dataset_42.csv\n",
      "dataset_43.csv\n",
      "dataset_44.csv\n",
      "dataset_45.csv\n",
      "dataset_46.csv\n",
      "dataset_47.csv\n",
      "dataset_48.csv\n"
     ]
    }
   ],
   "source": [
    "for dataset_index in range(1, 49):\n",
    "    filename = f'dataset_{dataset_index}.csv'\n",
    "    file_path = os.path.join('datasets/synthetic', filename)\n",
    "    print(filename)\n",
    "    # Load the dataset from the CSV file\n",
    "    dataset = np.loadtxt(file_path, delimiter=',')\n",
    "\n",
    "    # Split the dataset into X and y\n",
    "    X = dataset[:-1, :]\n",
    "    y = (dataset[-1, :]).reshape(-1,1)\n",
    "    \n",
    "    # Run simulation\n",
    "    results = single_dataset_simulation(X, y)\n",
    "    write_np_npz(filename[:-4] + \"_results\" + \".npz\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Local\\Temp\\ipykernel_2704\\1504324088.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_times = pd.concat([df_times, pd.DataFrame({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_2\n",
      "dataset_3\n",
      "dataset_4\n",
      "dataset_5\n",
      "dataset_6\n",
      "dataset_7\n",
      "dataset_8\n",
      "dataset_9\n",
      "dataset_10\n",
      "dataset_11\n",
      "dataset_12\n",
      "dataset_13\n",
      "dataset_14\n",
      "dataset_15\n",
      "dataset_16\n",
      "dataset_17\n",
      "dataset_18\n",
      "dataset_19\n",
      "dataset_20\n",
      "dataset_21\n",
      "dataset_22\n",
      "dataset_23\n",
      "dataset_24\n",
      "dataset_25\n",
      "dataset_26\n",
      "dataset_27\n",
      "dataset_28\n",
      "dataset_29\n",
      "dataset_30\n",
      "dataset_31\n",
      "dataset_32\n",
      "dataset_33\n",
      "dataset_34\n",
      "dataset_35\n",
      "dataset_36\n",
      "dataset_37\n",
      "dataset_38\n",
      "dataset_39\n",
      "dataset_40\n",
      "dataset_41\n",
      "dataset_42\n",
      "dataset_43\n",
      "dataset_44\n",
      "dataset_45\n",
      "dataset_46\n",
      "dataset_47\n",
      "dataset_48\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "df_times = pd.DataFrame(columns=[\"dataset\", \"iteration\", \"timing_approximated_ENRELM\", \"timing_incremental_ENRELM\", \"total_timing_ELM\", \"mean_loop_timing_ELM\"])\n",
    "first_idx = 1\n",
    "last_idx = 48\n",
    "for dataset_index in range(first_idx, last_idx+1):\n",
    "    filename = f'dataset_{dataset_index}.csv'\n",
    "    file_path = os.path.join('datasets/synthetic', filename)\n",
    "    # Load the dataset from the CSV file\n",
    "    dataset = np.loadtxt(file_path, delimiter=',')\n",
    "    print(filename[:-4])\n",
    "        \n",
    "    # Split the dataset into X and y\n",
    "    X = dataset[:, :-1]\n",
    "    y = (dataset[:, -1]).reshape(-1,1)\n",
    "    for iteration in range(iterations):\n",
    "        results = single_dataset_simulation(X, y)\n",
    "            # Append a new row to the dataframe with the results and dataset name\n",
    "        df_times = pd.concat([df_times, pd.DataFrame({\n",
    "            \"dataset\": [f'dataset_{dataset_index}'],\n",
    "            \"iteration\": [iteration],\n",
    "            \"timing_approximated_ENRELM\": [results[\"timing_approximated_ENRELM\"]],\n",
    "            \"timing_incremental_ENRELM\": [results[\"timing_incremental_ENRELM\"]],\n",
    "            \"total_timing_ELM\": [results[\"total_timing_ELM\"]],\n",
    "            \"mean_loop_timing_ELM\": [results[\"mean_loop_timing_ELM\"]]\n",
    "        })], ignore_index=True)\n",
    "    # saving results when computing times is disabled\n",
    "    #write_np_npz(filename[:-4] + \"_results\" + \".npz\", results)\n",
    "\n",
    "# Save times\n",
    "write_df_csv(\"times_synthetic\" + str(first_idx) + \"_\" + str(last_idx)+ \".csv\", df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
