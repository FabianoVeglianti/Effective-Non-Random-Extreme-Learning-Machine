{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure and subplots\n",
    "fig, axes = plt.subplots(4, 2, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "fig2, axes2 = plt.subplots(4, 2, figsize=(10, 10))\n",
    "fig2.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "datasets = {\n",
    "    1: \"Abalone\",\n",
    "    2: \"Auto MPG\",\n",
    "    3: \"California Housing\",\n",
    "    4: \"Delta Ailerons\",\n",
    "    5: \"LA Ozone\",\n",
    "    6: \"Machine CPU\",\n",
    "    7: \"Prostate Cancer\",\n",
    "    8: \"Servo\"\n",
    "}\n",
    "\n",
    "df_times = pd.DataFrame(columns=[\"dataset\", \"total_timing_ELM\", \"mean_loop_timing_ELM\", \"timing_approximated_ENRELM\", \"timing_incremental_ENRELM\"])\n",
    "dataset_index = 0\n",
    "for key, value in datasets.items():\n",
    "    dataset_index += 1\n",
    "    name = value\n",
    "\n",
    "\n",
    "    folder_path = os.path.join('results', 'datasets')\n",
    "    full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "\n",
    "    results = np.load(full_path)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot figures\n",
    "    ax = axes[(dataset_index-1)//2, (dataset_index-1)%2]\n",
    "    ax2 = axes2[(dataset_index-1)//2, (dataset_index-1)%2]\n",
    "\n",
    "    X_axis = np.arange(0, results['training_error_ELM'].shape[0]+1, 1)\n",
    "    # Training error plots (existing code)\n",
    "    ax.fill_between(X_axis, np.concatenate([np.array([1]), results[\"min_training_error_ELM\"]]), np.concatenate([np.array([1]), results[\"max_training_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "    ax.plot(X_axis, np.concatenate([np.array([1]), results[\"training_error_ELM\"]]), 'b-', label='ELM')\n",
    "    ax.plot(X_axis, np.concatenate([np.array([1]), results['training_error_approximated_ENRELM'][0:results['training_error_ELM'].shape[0]]]), 'r-', label='A-ENR-ELM')\n",
    "    non_zeros_incremental_ENRELM_training = results['training_error_incremental_ENRELM'][results['training_error_incremental_ENRELM'] != 0]\n",
    "    filled_incremental_ENRELM_training = np.ones(results['training_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_training.shape[0]) * non_zeros_incremental_ENRELM_training[-1]\n",
    "    ax.plot(X_axis[0:non_zeros_incremental_ENRELM_training.shape[0]], non_zeros_incremental_ENRELM_training, 'g-', label='training error incremental ENRELM')\n",
    "    ax.plot(X_axis[non_zeros_incremental_ENRELM_training.shape[0]:], filled_incremental_ENRELM_training, 'g--')\n",
    "    ax.set_ylim(ax.get_ylim()[0],ax.get_ylim()[1])\n",
    "\n",
    "    \n",
    "\n",
    "    # Test error plots with similar insets (new code for ax2)\n",
    "    ax2.fill_between(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"min_test_error_ELM\"]]), np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"max_test_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "    ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_ELM']]), 'b-', label='ELM')\n",
    "    ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_approximated_ENRELM'][0:results['test_error_ELM'].shape[0]]]), 'r-', label='A-ENR-ELM')\n",
    "    non_zeros_incremental_ENRELM_test = results['test_error_incremental_ENRELM'][results['test_error_incremental_ENRELM'] != 0]\n",
    "    filled_incremental_ENRELM_test = np.ones(results['test_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_test.shape[0]) * non_zeros_incremental_ENRELM_test[-1]\n",
    "    ax2.plot(X_axis[0:non_zeros_incremental_ENRELM_test.shape[0]], non_zeros_incremental_ENRELM_test, 'g-')\n",
    "    ax2.plot(X_axis[non_zeros_incremental_ENRELM_test.shape[0]:], filled_incremental_ENRELM_test, 'g--')\n",
    "    ax2.set_ylim(ax2.get_ylim()[0],ax2.get_ylim()[1])\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax2.set_title(name)\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax2.grid(True)\n",
    "\n",
    "\n",
    "folder_path = os.path.join('results', 'images')\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)  \n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=2, fontsize=12)\n",
    "fig2.legend(handles, labels, loc='lower center', ncol=2, fontsize=12)\n",
    "fig.subplots_adjust(bottom=0.1)\n",
    "#fig.savefig(os.path.join(folder_path, \"image_training_real_datasets.eps\"), format=\"eps\")\n",
    "fig.savefig(os.path.join(folder_path, \"image_training_real_datasets.png\"))\n",
    "\n",
    "fig2.subplots_adjust(bottom=0.1)\n",
    "#fig2.savefig(os.path.join(folder_path, \"image2_test_real_datasets.eps\"), format=\"eps\")\n",
    "fig2.savefig(os.path.join(folder_path, \"image2_test_real_datasets.png\"))\n",
    "fig.show()\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv(file_path, T=None, n0=None, X_distribution=None, X_range=None, \n",
    "               X_cov=None, X_rho=None, y_function=None, y_terms=None, y_SNR=None):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, sep=';')  # Assuming ';' is the delimiter \n",
    "    \n",
    "    # Applying filters based on provided parameters\n",
    "    filters = {\n",
    "        'T': T,\n",
    "        'n0': n0,\n",
    "        'X_distribution': X_distribution,\n",
    "        'X_range': X_range,\n",
    "        'X_cov': X_cov,\n",
    "        'X_rho': X_rho,\n",
    "        'y_function': y_function,\n",
    "        'y_terms': y_terms,\n",
    "        'y_SNR': y_SNR\n",
    "    }\n",
    "    \n",
    "    # Remove None values from filters\n",
    "    filters = {key: value for key, value in filters.items() if value is not None}\n",
    "    #print(filters)\n",
    "    # Filter the DataFrame\n",
    "    for column, value in filters.items():\n",
    "        #print(column + \"\\t\\t\" + str(value))\n",
    "        df = df[df[column] == value]\n",
    "\n",
    "    if len(df['index'].tolist()) > 1:\n",
    "        logger.error(\"Found more than one synthetic dataset matching filter, unable to identify a single dataset\")\n",
    "    elif len(df['index'].tolist()) == 0:\n",
    "        logger.error(\"Found 0 synthetic dataset matching filter\")\n",
    "\n",
    "    # Return the index of the filtered rows\n",
    "    return df['index'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_synthetic(first_idx = 1, last_idx = 12, T=300, n0 = 20):\n",
    "    # Initialize figure and subplots\n",
    "    fig, axes = plt.subplots(6, 2, figsize=(12, 12.5))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    fig2, axes2 = plt.subplots(6, 2, figsize=(12, 12.5))\n",
    "    fig2.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "\n",
    "    # Define conditions\n",
    "    y_SNR_values = [2, 10]\n",
    "    y_functions = [\"linear\", \"shallow\"]\n",
    "    X_distributions = [\"uniform\", \"gaussian\"]\n",
    "    X_covs = [\"iid\", \"toeplix\"]\n",
    "\n",
    "    # Titles for columns\n",
    "    column_titles = [\"SNR = 2\", \"SNR = 10\"]\n",
    "\n",
    "    # Add column titles\n",
    "    for i, title in enumerate(column_titles):\n",
    "        fig.text(0.25 + i * 0.5, 0.92, title, ha='center', fontsize=14)\n",
    "        fig2.text(0.25 + i * 0.5, 0.92, title, ha='center', fontsize=14)\n",
    "\n",
    "    fig.text(0.02, 0.7, 'Linear Function', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "    fig.text(0.02, 0.3, 'Shallow NN Function', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "    fig2.text(0.02, 0.7, 'Linear Function', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "    fig2.text(0.02, 0.3, 'Shallow NN Function', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate through each combination to place in the correct subplot\n",
    "    for i, y_SNR in enumerate(y_SNR_values):\n",
    "        for j, y_function in enumerate(y_functions):\n",
    "            for k, X_distribution in enumerate(X_distributions):\n",
    "                if X_distribution == \"uniform\":\n",
    "                    row = 0\n",
    "                    X_cov = \"//\"\n",
    "                    title = r\"$X \\sim \\text{Unif}(-2\\pi, 2\\pi)$\"\n",
    "                    col = 0 if y_SNR == 2 else 1\n",
    "\n",
    "                    if col == 0:  # Only place it for the first column to avoid repetition\n",
    "                        fig.text(0.06, 0.83 - (3 * j + row) * 0.135, title, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "                        fig2.text(0.06, 0.83 - (3 * j + row) * 0.135, title, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "                    ax = axes[row + 3*j, col]\n",
    "\n",
    "                    dataset_index = filter_csv(\"synthetic_datasets_description.csv\", T, n0, X_distribution, X_cov = X_cov, y_function= y_function, y_SNR=y_SNR)\n",
    "                    name = \"dataset_\" + str(dataset_index)\n",
    "                    folder_path = os.path.join('results', 'datasets')\n",
    "                    full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "                    results = np.load(full_path)\n",
    "\n",
    "                    # Plot figures\n",
    "                    ax = axes[row + 3*j, col]\n",
    "                    ax2 = axes2[row + 3*j, col]\n",
    "\n",
    "                    X_axis = np.arange(0, results['training_error_ELM'].shape[0]+1, 1)\n",
    "                    # Training error plots (existing code)\n",
    "                    ax.fill_between(X_axis, np.concatenate([np.array([1]), results[\"min_training_error_ELM\"]]), np.concatenate([np.array([1]), results[\"max_training_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "                    ax.plot(X_axis, np.concatenate([np.array([1]), results[\"training_error_ELM\"]]), 'b-', label='ELM')\n",
    "                    ax.plot(X_axis, np.concatenate([np.array([1]), results['training_error_approximated_ENRELM'][0:results['training_error_ELM'].shape[0]]]), 'r-', label='A-ENR-ELM')\n",
    "                    non_zeros_incremental_ENRELM_training = results['training_error_incremental_ENRELM'][results['training_error_incremental_ENRELM'] != 0]\n",
    "                    filled_incremental_ENRELM_training = np.ones(results['training_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_training.shape[0]) * non_zeros_incremental_ENRELM_training[-1]\n",
    "                    ax.plot(X_axis[0:non_zeros_incremental_ENRELM_training.shape[0]], non_zeros_incremental_ENRELM_training, 'g-', label='I-ENR-ELM')\n",
    "                    ax.plot(X_axis[non_zeros_incremental_ENRELM_training.shape[0]:], filled_incremental_ENRELM_training, 'g--')\n",
    "                    ax.set_ylim(ax.get_ylim()[0],ax.get_ylim()[1])\n",
    "\n",
    "                    \n",
    "                    #for ELM and A-ENRELM the first error is set equal to the first of I-ENRELM since it represent the usage of mean(y_train) as predictor\n",
    "                    # Test error plots with similar insets (new code for ax2)\n",
    "                    ax2.fill_between(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"min_test_error_ELM\"]]), np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"max_test_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "                    ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_ELM']]), 'b-', label='ELM')\n",
    "                    ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_approximated_ENRELM'][0:results['test_error_ELM'].shape[0]]]), 'r-', label='A-ENR-ELM')\n",
    "                    non_zeros_incremental_ENRELM_test = results['test_error_incremental_ENRELM'][results['test_error_incremental_ENRELM'] != 0]\n",
    "                    filled_incremental_ENRELM_test = np.ones(results['test_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_test.shape[0]) * non_zeros_incremental_ENRELM_test[-1]\n",
    "                    ax2.plot(X_axis[0:non_zeros_incremental_ENRELM_test.shape[0]], non_zeros_incremental_ENRELM_test, 'g-', label = \"I-ENR-ELM\")\n",
    "                    ax2.plot(X_axis[non_zeros_incremental_ENRELM_test.shape[0]:], filled_incremental_ENRELM_test, 'g--')\n",
    "                    ax2.set_ylim(ax2.get_ylim()[0],ax2.get_ylim()[1])\n",
    "\n",
    "\n",
    "\n",
    "                    ax.set_title(name)\n",
    "                    ax.set_ylabel('RMSE')\n",
    "                    ax.grid(True)\n",
    "\n",
    "                    ax2.set_title(name)\n",
    "                    ax2.set_ylabel('RMSE')\n",
    "                    ax2.grid(True)\n",
    "\n",
    "\n",
    "                elif X_distribution == \"gaussian\":\n",
    "                    for l, X_cov in enumerate(X_covs):\n",
    "                        if X_cov == \"iid\":\n",
    "                            row = 1\n",
    "                            title = r\"$X \\sim \\mathcal{N}(0, I)$\"\n",
    "                        elif X_cov == \"toeplix\":\n",
    "                            row = 2\n",
    "                            title = r\"$X \\sim \\mathcal{N}(0, \\text{Toeplix})$\"\n",
    "                        col = 0 if y_SNR == 2 else 1\n",
    "\n",
    "                        # Place the row title, rotated vertically, on the left of both figures\n",
    "                        if col == 0:  # Only place it for the first column to avoid repetition\n",
    "                            fig.text(0.06, 0.83 - (3 * j + row) * 0.135, title, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "                            fig2.text(0.06, 0.83 - (3 * j + row) * 0.135, title, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "\n",
    "                        dataset_index = filter_csv(\"synthetic_datasets_description.csv\", T, n0, X_distribution, X_cov = X_cov, y_function= y_function, y_SNR=y_SNR)\n",
    "                        name = \"dataset_\" + str(dataset_index)\n",
    "                        folder_path = os.path.join('results', 'datasets')\n",
    "                        full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "                        results = np.load(full_path)\n",
    "                        \n",
    "                        # Plot figures\n",
    "                        ax = axes[row + 3*j, col]\n",
    "                        ax2 = axes2[row + 3*j, col]\n",
    "\n",
    "                        X_axis = np.arange(0, results['training_error_ELM'].shape[0]+1, 1)\n",
    "\n",
    "                        # Training error plots (existing code)\n",
    "                        ax.fill_between(X_axis, np.concatenate([np.array([1]), results[\"min_training_error_ELM\"]]), np.concatenate([np.array([1]), results[\"max_training_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "                        ax.plot(X_axis, np.concatenate([np.array([1]), results[\"training_error_ELM\"]]), 'b-', label='ELM')\n",
    "                        ax.plot(X_axis, np.concatenate([np.array([1]), results['training_error_approximated_ENRELM']]), 'r-', label='A-ENR-ELM')\n",
    "                        non_zeros_incremental_ENRELM_training = results['training_error_incremental_ENRELM'][results['training_error_incremental_ENRELM'] != 0]\n",
    "                        filled_incremental_ENRELM_training = np.ones(results['training_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_training.shape[0]) * non_zeros_incremental_ENRELM_training[-1]\n",
    "                        ax.plot(X_axis[0:non_zeros_incremental_ENRELM_training.shape[0]], non_zeros_incremental_ENRELM_training, 'g-', label='I-ENR-ELM')\n",
    "                        ax.plot(X_axis[non_zeros_incremental_ENRELM_training.shape[0]:], filled_incremental_ENRELM_training, 'g--')\n",
    "                        ax.set_ylim(ax.get_ylim()[0],ax.get_ylim()[1])\n",
    "\n",
    "                        \n",
    "                        #for ELM and A-ENRELM the first error is set equal to the first of I-ENRELM since it represent the usage of mean(y_train) as predictor\n",
    "                        # Test error plots with similar insets (new code for ax2)\n",
    "                        ax2.fill_between(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"min_test_error_ELM\"]]), np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"max_test_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "                        ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_ELM']]), 'b-', label='ELM')\n",
    "                        ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_approximated_ENRELM']]), 'r-', label='A-ENR-ELM')\n",
    "                        non_zeros_incremental_ENRELM_test = results['test_error_incremental_ENRELM'][results['test_error_incremental_ENRELM'] != 0]\n",
    "                        filled_incremental_ENRELM_test = np.ones(results['test_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_test.shape[0]) * non_zeros_incremental_ENRELM_test[-1]\n",
    "                        ax2.plot(X_axis[0:non_zeros_incremental_ENRELM_test.shape[0]], non_zeros_incremental_ENRELM_test, 'g-', label = \"I-ENR-ELM\")\n",
    "                        ax2.plot(X_axis[non_zeros_incremental_ENRELM_test.shape[0]:], filled_incremental_ENRELM_test, 'g--')\n",
    "                        ax2.set_ylim(ax2.get_ylim()[0],ax2.get_ylim()[1])\n",
    "\n",
    "\n",
    "\n",
    "                        ax.set_title(name)\n",
    "                        ax.set_ylabel('RMSE')\n",
    "                        ax.grid(True)\n",
    "\n",
    "                        ax2.set_title(name)\n",
    "                        ax2.set_ylabel('RMSE')\n",
    "                        ax2.grid(True)\n",
    "                    \n",
    "\n",
    "    folder_path = os.path.join('results', 'images')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)  \n",
    "    folder_path = os.path.join('results', 'images')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=2, fontsize=12)\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    fig2.legend(handles2, labels2, loc='lower center', ncol=2, fontsize=12)\n",
    "    fig.subplots_adjust(bottom=0.1)\n",
    "    #fig.savefig(os.path.join(folder_path, \"image_training_synthetic_datasets_\" + str(n0) + \".eps\"), format=\"eps\")\n",
    "    fig.savefig(os.path.join(folder_path, \"image_training_synthetic_datasets_\" + str(T) + \"_\" + str(n0) + \".png\"))\n",
    "\n",
    "    fig2.subplots_adjust(bottom=0.1)\n",
    "    #fig2.savefig(os.path.join(folder_path, \"image2_test_synthetic_datasets_\" + str(n0) + \".eps\"), format=\"eps\")\n",
    "    fig2.savefig(os.path.join(folder_path, \"image2_test_synthetic_datasets_\" + str(T) + \"_\" + str(n0) + \".png\"))\n",
    "    fig.show()\n",
    "    fig2.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_synthetic(first_idx = 1, last_idx = 12, T = 300, n0 = 20)\n",
    "plot_synthetic(first_idx = 13, last_idx = 24, T = 300, n0 = 80)\n",
    "plot_synthetic(first_idx = 25, last_idx = 36, T = 1200, n0 = 20)\n",
    "plot_synthetic(first_idx = 37, last_idx = 48, T = 1200, n0 = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = {\n",
    "    1: \"Abalone\",\n",
    "    2: \"Auto MPG\",\n",
    "    3: \"California Housing\",\n",
    "    4: \"Delta Ailerons\",\n",
    "    5: \"LA Ozone\",\n",
    "    6: \"Machine CPU\",\n",
    "    7: \"Prostate Cancer\",\n",
    "    8: \"Servo\"\n",
    "}\n",
    "\n",
    "# Columns we want to populate in the DataFrame\n",
    "metrics = [\n",
    "    'test_error_approximated_ENRELM', \n",
    "    'test_error_incremental_ENRELM', \n",
    "    'test_error_ELM'\n",
    "]\n",
    "\n",
    "# Generate new column names for each metric\n",
    "columns = ['dataset']\n",
    "for metric in metrics:\n",
    "    columns.append(f\"{metric}_n\")\n",
    "    columns.append(f\"{metric}_value\")\n",
    "\n",
    "# Initialize an empty DataFrame to store results\n",
    "df = pd.DataFrame(index=datasets.keys(), columns=columns)\n",
    "\n",
    "# Iterate through each dataset\n",
    "for key, value in datasets.items():\n",
    "    # Extract the dataset name\n",
    "    name = value\n",
    "    \n",
    "    # Load the results for the dataset\n",
    "    folder_path = os.path.join('results', 'datasets')\n",
    "    full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "    results = np.load(full_path)\n",
    "    df.loc[key, 'dataset'] = name\n",
    "    \n",
    "    # Populate the DataFrame for each metric\n",
    "    for metric in metrics:\n",
    "        # Extract the array for the current metric\n",
    "        metric_values = results[metric]\n",
    "        \n",
    "        # Handle ELM metrics differently as they involve mean and std\n",
    "        if \"training_error_ELM\" in metric or \"test_error_ELM\" in metric:\n",
    "            std_key = \"std_\" + metric\n",
    "            std_values = results[std_key]\n",
    "            min_index = np.argmin(metric_values)\n",
    "            min_value = metric_values[min_index]\n",
    "            std_value = std_values[min_index]\n",
    "            # Set values for the new columns\n",
    "            df.loc[key, f\"{metric}_n\"] = min_index\n",
    "            df.loc[key, f\"{metric}_value\"] = f\"{min_value:.4f} +/- {std_value:.4f}\"\n",
    "        else:\n",
    "            # For other metrics, simply find the argmin and min\n",
    "            min_index = np.argmin(metric_values)\n",
    "            min_value = metric_values[min_index]\n",
    "            # Set values for the new columns\n",
    "            df.loc[key, f\"{metric}_n\"] = min_index\n",
    "            df.loc[key, f\"{metric}_value\"] = f\"{min_value:.4f}\"\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "folder_path = os.path.join('results', 'tables')\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)  \n",
    "df.to_csv(os.path.join(folder_path, \"results_table_real.csv\"), index=False)\n",
    "\n",
    "# Print the DataFrame to check the output\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_table_synthetic(first_idx, last_idx):\n",
    "    # Columns we want to populate in the DataFrame\n",
    "    metrics = [\n",
    "        'test_error_approximated_ENRELM', \n",
    "        'test_error_incremental_ENRELM', \n",
    "        'test_error_ELM'\n",
    "    ]\n",
    "\n",
    "    # Generate new column names for each metric\n",
    "    columns = ['dataset']\n",
    "    for metric in metrics:\n",
    "        columns.append(f\"{metric}_n\")\n",
    "        columns.append(f\"{metric}_value\")\n",
    "\n",
    "    dataset_names = []\n",
    "    for idx in range(first_idx, last_idx+1):\n",
    "        # Extract the dataset name\n",
    "        name = \"dataset_\" + str(idx)\n",
    "        dataset_names.append(name)\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    df = pd.DataFrame(index=dataset_names, columns=columns)\n",
    "\n",
    "    # Iterate through each dataset\n",
    "    for name in dataset_names:\n",
    "        # Load the results for the dataset\n",
    "        folder_path = os.path.join('results', 'datasets')\n",
    "        full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "        results = np.load(full_path)\n",
    "        df.loc[name, 'dataset'] = name\n",
    "        \n",
    "        # Populate the DataFrame for each metric\n",
    "        for metric in metrics:\n",
    "            # Extract the array for the current metric\n",
    "            metric_values = results[metric]\n",
    "            \n",
    "            # Handle ELM metrics differently as they involve mean and std\n",
    "            if \"training_error_ELM\" in metric or \"test_error_ELM\" in metric:\n",
    "                std_key = \"std_\" + metric\n",
    "                std_values = results[std_key]\n",
    "                min_index = np.argmin(metric_values)\n",
    "                min_value = metric_values[min_index]\n",
    "                std_value = std_values[min_index]\n",
    "                # Set values for the new columns\n",
    "                df.loc[name, f\"{metric}_n\"] = min_index\n",
    "                df.loc[name, f\"{metric}_value\"] = f\"{min_value:.4f} +/- {std_value:.4f}\"\n",
    "            else:\n",
    "                # For other metrics, simply find the argmin and min\n",
    "                min_index = np.argmin(metric_values)\n",
    "                min_value = metric_values[min_index]\n",
    "                # Set values for the new columns\n",
    "                df.loc[name, f\"{metric}_n\"] = min_index\n",
    "                df.loc[name, f\"{metric}_value\"] = f\"{min_value:.4f}\"\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    folder_path = os.path.join('results', 'tables')\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)  \n",
    "    df.to_csv(os.path.join(folder_path, \"results_table_synthetic\" + str(first_idx) + \"_\" + str(last_idx)+ \".csv\"), index=False)\n",
    "\n",
    "    # Print the DataFrame to check the output\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_table_synthetic(first_idx = 1, last_idx = 12)\n",
    "performance_table_synthetic(first_idx = 13, last_idx = 24)\n",
    "performance_table_synthetic(first_idx = 25, last_idx = 36)\n",
    "performance_table_synthetic(first_idx = 37, last_idx = 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the compressed file containing the dataframe\n",
    "folder_path = os.path.join('results', 'datasets')\n",
    "filename = \"times_real.csv\"\n",
    "full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "\n",
    "# Extracting the dataframe from the loaded data\n",
    "df_times = pd.read_csv(full_path, sep=\";\").drop(columns=[\"iteration\"])\n",
    "\n",
    "# Grouping by 'dataset' and calculating the mean for all other columns\n",
    "df_means = df_times.groupby(\"dataset\", as_index=False).mean()\n",
    "\n",
    "folder_path = os.path.join('results', 'tables')\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)  \n",
    "df_means.to_csv(os.path.join(folder_path, \"times_table_real.csv\"), index=False)\n",
    "print(df_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the compressed file containing the dataframe\n",
    "folder_path = os.path.join('results', 'datasets')\n",
    "first_idx = 1\n",
    "last_idx = 48\n",
    "filename = \"times_synthetic\"+ str(first_idx) + \"_\" + str(last_idx)+\".csv\"\n",
    "full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "\n",
    "# Extracting the dataframe from the loaded data\n",
    "df_times = pd.read_csv(full_path, sep=\";\").drop(columns=[\"iteration\"])\n",
    "\n",
    "# Grouping by 'dataset' and calculating the mean for all other columns\n",
    "df_means = df_times.groupby(\"dataset\", as_index=False).mean()\n",
    "\n",
    "# Extract the number from each dataset and convert it to an integer\n",
    "df_means['number'] = df_means['dataset'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Sort the dataframe based on the extracted number\n",
    "df_sorted = df_means.sort_values('number').drop('number', axis=1)\n",
    "\n",
    "# Reset index\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "folder_path = os.path.join('results', 'tables')\n",
    "if not os.path.exists(folder_path):\n",
    "    os.mkdir(folder_path)  \n",
    "df_sorted.to_csv(os.path.join(folder_path, \"times_table_synthetic\" + str(first_idx) + \"_\" + str(last_idx)+ \".csv\"), index=False)\n",
    "print(df_sorted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
