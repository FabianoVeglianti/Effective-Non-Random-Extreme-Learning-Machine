{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure and subplots\n",
    "fig, axes = plt.subplots(5, 2, figsize=(10, 12.5))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "fig2, axes2 = plt.subplots(5, 2, figsize=(10, 12.5))\n",
    "fig2.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "datasets = {\n",
    "    1: \"Abalone\",\n",
    "    2: \"Auto MPG\",\n",
    "    3: \"Bank\",\n",
    "    4: \"California Housing\",\n",
    "    5: \"Delta Ailerons\",\n",
    "    6: \"LA Ozone\",\n",
    "    7: \"Machine CPU\",\n",
    "    8: \"Prostate Cancer\",\n",
    "    9: \"Servo\"\n",
    "}\n",
    "\n",
    "df_times = pd.DataFrame(columns=[\"dataset\", \"total_timing_ELM\", \"mean_loop_timing_ELM\", \"timing_approximated_ENRELM\", \"timing_incremental_ENRELM\"])\n",
    "dataset_index = 0\n",
    "for key, value in datasets.items():\n",
    "    dataset_index += 1\n",
    "    name = value\n",
    "\n",
    "\n",
    "    folder_path = 'results/datasets'\n",
    "    full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "\n",
    "    results = np.load(full_path)\n",
    "\n",
    "\n",
    "\n",
    "    # Plot figures\n",
    "    ax = axes[(dataset_index-1)//2, (dataset_index-1)%2]\n",
    "    ax2 = axes2[(dataset_index-1)//2, (dataset_index-1)%2]\n",
    "\n",
    "    X_axis = np.arange(0, results['training_error_ELM'].shape[0]+1, 1)\n",
    "    # Training error plots (existing code)\n",
    "    ax.fill_between(X_axis, np.concatenate([np.array([1]), results[\"min_training_error_ELM\"]]), np.concatenate([np.array([1]), results[\"max_training_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "    ax.plot(X_axis, np.concatenate([np.array([1]), results[\"training_error_ELM\"]]), 'b-', label='training error ELM')\n",
    "    ax.plot(X_axis, results['training_error_approximated_ENRELM'][0:results['training_error_ELM'].shape[0]+1], 'r-', label='training error approximated ENRELM')\n",
    "    non_zeros_incremental_ENRELM_training = results['training_error_incremental_ENRELM'][results['training_error_incremental_ENRELM'] != 0]\n",
    "    filled_incremental_ENRELM_training = np.ones(results['training_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_training.shape[0]) * non_zeros_incremental_ENRELM_training[-1]\n",
    "    ax.plot(X_axis[0:non_zeros_incremental_ENRELM_training.shape[0]], non_zeros_incremental_ENRELM_training, 'g-', label='training error incremental ENRELM')\n",
    "    ax.plot(X_axis[non_zeros_incremental_ENRELM_training.shape[0]:], filled_incremental_ENRELM_training, 'g--')\n",
    "    ax.set_ylim(ax.get_ylim()[0],ax.get_ylim()[1])\n",
    "\n",
    "    \n",
    "\n",
    "    # Test error plots with similar insets (new code for ax2)\n",
    "    ax2.fill_between(X_axis, np.concatenate([np.array([1]), results[\"min_test_error_ELM\"]]), np.concatenate([np.array([1]), results[\"max_test_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "    ax2.plot(X_axis, np.concatenate([np.array([1]), results['test_error_ELM']]), 'b-', label='testing error ELM')\n",
    "    ax2.plot(X_axis, results['test_error_approximated_ENRELM'][0:results['test_error_ELM'].shape[0]+1], 'r-', label='testing error approximated ENRELM')\n",
    "    non_zeros_incremental_ENRELM_test = results['test_error_incremental_ENRELM'][results['test_error_incremental_ENRELM'] != 0]\n",
    "    filled_incremental_ENRELM_test = np.ones(results['test_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_test.shape[0]) * non_zeros_incremental_ENRELM_test[-1]\n",
    "    ax2.plot(X_axis[0:non_zeros_incremental_ENRELM_test.shape[0]], non_zeros_incremental_ENRELM_test, 'g-')\n",
    "    ax2.plot(X_axis[non_zeros_incremental_ENRELM_test.shape[0]:], filled_incremental_ENRELM_test, 'g--')\n",
    "    ax2.set_ylim(ax2.get_ylim()[0],ax2.get_ylim()[1])\n",
    "\n",
    "\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax2.set_title(name)\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax2.grid(True)\n",
    "\n",
    "\n",
    "folder_path = \"results/images\"\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', ncol=2, fontsize=12)\n",
    "fig2.legend(handles, labels, loc='lower center', ncol=2, fontsize=12)\n",
    "fig.subplots_adjust(bottom=0.1)\n",
    "#fig.savefig(os.path.join(folder_path, \"image_training_real_datasets.eps\"), format=\"eps\")\n",
    "#fig.savefig(os.path.join(folder_path, \"image_training_real_datasets.png\"))\n",
    "\n",
    "fig2.subplots_adjust(bottom=0.1)\n",
    "#fig2.savefig(os.path.join(folder_path, \"image2_test_real_datasets.eps\"), format=\"eps\")\n",
    "#fig2.savefig(os.path.join(folder_path, \"image2_test_real_datasets.png\"))\n",
    "fig.show()\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv(file_path, T=None, n0=None, X_distribution=None, X_range=None, \n",
    "               X_cov=None, X_rho=None, y_function=None, y_terms=None, y_SNR=None):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path, sep=';')  # Assuming ';' is the delimiter \n",
    "    \n",
    "    # Applying filters based on provided parameters\n",
    "    filters = {\n",
    "        'T': T,\n",
    "        'n0': n0,\n",
    "        'X_distribution': X_distribution,\n",
    "        'X_range': X_range,\n",
    "        'X_cov': X_cov,\n",
    "        'X_rho': X_rho,\n",
    "        'y_function': y_function,\n",
    "        'y_terms': y_terms,\n",
    "        'y_SNR': y_SNR\n",
    "    }\n",
    "    \n",
    "    # Remove None values from filters\n",
    "    filters = {key: value for key, value in filters.items() if value is not None}\n",
    "    #print(filters)\n",
    "    # Filter the DataFrame\n",
    "    for column, value in filters.items():\n",
    "        #print(column + \"\\t\\t\" + str(value))\n",
    "        df = df[df[column] == value]\n",
    "\n",
    "    if len(df['index'].tolist()) > 1:\n",
    "        logger.error(\"Found more than one synthetic dataset matching filter, unable to identify a single dataset\")\n",
    "    elif len(df['index'].tolist()) == 0:\n",
    "        logger.error(\"Found 0 synthetic dataset matching filter\")\n",
    "\n",
    "    # Return the index of the filtered rows\n",
    "    return df['index'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_synthetic(first_idx = 1, last_idx = 12, T=300, n0 = 20):\n",
    "    # Initialize figure and subplots\n",
    "    fig, axes = plt.subplots(6, 2, figsize=(12, 12.5))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    fig2, axes2 = plt.subplots(6, 2, figsize=(12, 12.5))\n",
    "    fig2.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "\n",
    "    # Define conditions\n",
    "    y_SNR_values = [2, 10]\n",
    "    y_functions = [\"linear\", \"shallow\"]\n",
    "    X_distributions = [\"uniform\", \"gaussian\"]\n",
    "    X_covs = [\"iid\", \"toeplix\"]\n",
    "\n",
    "    # Titles for columns\n",
    "    column_titles = [\"SNR = 2\", \"SNR = 10\"]\n",
    "\n",
    "    # Add column titles\n",
    "    for i, title in enumerate(column_titles):\n",
    "        fig.text(0.25 + i * 0.5, 0.92, title, ha='center', fontsize=14)\n",
    "        fig2.text(0.25 + i * 0.5, 0.92, title, ha='center', fontsize=14)\n",
    "\n",
    "    fig.text(0.02, 0.7, 'Linear Function', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "    fig.text(0.02, 0.3, 'Shallow NN Function', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "    fig2.text(0.02, 0.7, 'Linear Function', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "    fig2.text(0.02, 0.3, 'Shallow NN Function', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "    # Iterate through each combination to place in the correct subplot\n",
    "    for i, y_SNR in enumerate(y_SNR_values):\n",
    "        for j, y_function in enumerate(y_functions):\n",
    "            for k, X_distribution in enumerate(X_distributions):\n",
    "                if X_distribution == \"uniform\":\n",
    "                    row = 0\n",
    "                    X_cov = \"//\"\n",
    "                    title = r\"$X \\sim \\text{Unif}(-2\\pi, 2\\pi)$\"\n",
    "                    col = 0 if y_SNR == 2 else 1\n",
    "\n",
    "                    if col == 0:  # Only place it for the first column to avoid repetition\n",
    "                        fig.text(0.06, 0.83 - (3 * j + row) * 0.135, title, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "                        fig2.text(0.06, 0.83 - (3 * j + row) * 0.135, title, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "                    ax = axes[row + 3*j, col]\n",
    "\n",
    "                    dataset_index = filter_csv(\"synthetic_datasets_description.csv\", T, n0, X_distribution, X_cov = X_cov, y_function= y_function, y_SNR=y_SNR)\n",
    "                    name = \"dataset_\" + str(dataset_index)\n",
    "                    folder_path = 'results/datasets'\n",
    "                    full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "                    results = np.load(full_path)\n",
    "\n",
    "                    # Plot figures\n",
    "                    ax = axes[row + 3*j, col]\n",
    "                    ax2 = axes2[row + 3*j, col]\n",
    "\n",
    "                    X_axis = np.arange(0, results['training_error_ELM'].shape[0]+1, 1)\n",
    "                    # Training error plots (existing code)\n",
    "                    ax.fill_between(X_axis, np.concatenate([np.array([1]), results[\"min_training_error_ELM\"]]), np.concatenate([np.array([1]), results[\"max_training_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "                    ax.plot(X_axis, np.concatenate([np.array([1]), results[\"training_error_ELM\"]]), 'b-', label='ELM')\n",
    "                    ax.plot(X_axis, np.concatenate([np.array([1]), results['training_error_approximated_ENRELM'][0:results['training_error_ELM'].shape[0]]]), 'r-', label='A-ENR-ELM')\n",
    "                    non_zeros_incremental_ENRELM_training = results['training_error_incremental_ENRELM'][results['training_error_incremental_ENRELM'] != 0]\n",
    "                    filled_incremental_ENRELM_training = np.ones(results['training_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_training.shape[0]) * non_zeros_incremental_ENRELM_training[-1]\n",
    "                    ax.plot(X_axis[0:non_zeros_incremental_ENRELM_training.shape[0]], non_zeros_incremental_ENRELM_training, 'g-', label='I-ENR-ELM')\n",
    "                    ax.plot(X_axis[non_zeros_incremental_ENRELM_training.shape[0]:], filled_incremental_ENRELM_training, 'g--')\n",
    "                    ax.set_ylim(ax.get_ylim()[0],ax.get_ylim()[1])\n",
    "\n",
    "                    \n",
    "                    #for ELM and A-ENRELM the first error is set equal to the first of I-ENRELM since it represent the usage of mean(y_train) as predictor\n",
    "                    # Test error plots with similar insets (new code for ax2)\n",
    "                    ax2.fill_between(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"min_test_error_ELM\"]]), np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"max_test_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "                    ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_ELM']]), 'b-', label='ELM')\n",
    "                    ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_approximated_ENRELM'][0:results['test_error_ELM'].shape[0]]]), 'r-', label='A-ENR-ELM')\n",
    "                    non_zeros_incremental_ENRELM_test = results['test_error_incremental_ENRELM'][results['test_error_incremental_ENRELM'] != 0]\n",
    "                    filled_incremental_ENRELM_test = np.ones(results['test_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_test.shape[0]) * non_zeros_incremental_ENRELM_test[-1]\n",
    "                    ax2.plot(X_axis[0:non_zeros_incremental_ENRELM_test.shape[0]], non_zeros_incremental_ENRELM_test, 'g-', label = \"I-ENR-ELM\")\n",
    "                    ax2.plot(X_axis[non_zeros_incremental_ENRELM_test.shape[0]:], filled_incremental_ENRELM_test, 'g--')\n",
    "                    ax2.set_ylim(ax2.get_ylim()[0],ax2.get_ylim()[1])\n",
    "\n",
    "\n",
    "\n",
    "                    ax.set_title(name)\n",
    "                    ax.set_ylabel('RMSE')\n",
    "                    ax.grid(True)\n",
    "\n",
    "                    ax2.set_title(name)\n",
    "                    ax2.set_ylabel('RMSE')\n",
    "                    ax2.grid(True)\n",
    "\n",
    "\n",
    "                elif X_distribution == \"gaussian\":\n",
    "                    for l, X_cov in enumerate(X_covs):\n",
    "                        if X_cov == \"iid\":\n",
    "                            row = 1\n",
    "                            title = r\"$X \\sim \\mathcal{N}(0, I)$\"\n",
    "                        elif X_cov == \"toeplix\":\n",
    "                            row = 2\n",
    "                            title = r\"$X \\sim \\mathcal{N}(0, \\text{Toeplix})$\"\n",
    "                        col = 0 if y_SNR == 2 else 1\n",
    "\n",
    "                        # Place the row title, rotated vertically, on the left of both figures\n",
    "                        if col == 0:  # Only place it for the first column to avoid repetition\n",
    "                            fig.text(0.06, 0.83 - (3 * j + row) * 0.135, title, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "                            fig2.text(0.06, 0.83 - (3 * j + row) * 0.135, title, ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "\n",
    "                        dataset_index = filter_csv(\"synthetic_datasets_description.csv\", T, n0, X_distribution, X_cov = X_cov, y_function= y_function, y_SNR=y_SNR)\n",
    "                        name = \"dataset_\" + str(dataset_index)\n",
    "                        folder_path = 'results/datasets'\n",
    "                        full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "                        results = np.load(full_path)\n",
    "                        \n",
    "                        # Plot figures\n",
    "                        ax = axes[row + 3*j, col]\n",
    "                        ax2 = axes2[row + 3*j, col]\n",
    "\n",
    "                        X_axis = np.arange(0, results['training_error_ELM'].shape[0]+1, 1)\n",
    "\n",
    "                        # Training error plots (existing code)\n",
    "                        ax.fill_between(X_axis, np.concatenate([np.array([1]), results[\"min_training_error_ELM\"]]), np.concatenate([np.array([1]), results[\"max_training_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "                        ax.plot(X_axis, np.concatenate([np.array([1]), results[\"training_error_ELM\"]]), 'b-', label='ELM')\n",
    "                        ax.plot(X_axis, np.concatenate([np.array([1]), results['training_error_approximated_ENRELM'][0:results['training_error_ELM'].shape[0]]]), 'r-', label='A-ENR-ELM')\n",
    "                        non_zeros_incremental_ENRELM_training = results['training_error_incremental_ENRELM'][results['training_error_incremental_ENRELM'] != 0]\n",
    "                        filled_incremental_ENRELM_training = np.ones(results['training_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_training.shape[0]) * non_zeros_incremental_ENRELM_training[-1]\n",
    "                        ax.plot(X_axis[0:non_zeros_incremental_ENRELM_training.shape[0]], non_zeros_incremental_ENRELM_training, 'g-', label='I-ENR-ELM')\n",
    "                        ax.plot(X_axis[non_zeros_incremental_ENRELM_training.shape[0]:], filled_incremental_ENRELM_training, 'g--')\n",
    "                        ax.set_ylim(ax.get_ylim()[0],ax.get_ylim()[1])\n",
    "\n",
    "                        \n",
    "                        #for ELM and A-ENRELM the first error is set equal to the first of I-ENRELM since it represent the usage of mean(y_train) as predictor\n",
    "                        # Test error plots with similar insets (new code for ax2)\n",
    "                        ax2.fill_between(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"min_test_error_ELM\"]]), np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results[\"max_test_error_ELM\"]]), color='blue', alpha=0.2)\n",
    "                        ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_ELM']]), 'b-', label='ELM')\n",
    "                        ax2.plot(X_axis, np.concatenate([np.array([results['test_error_incremental_ENRELM'][0]]), results['test_error_approximated_ENRELM'][0:results['test_error_ELM'].shape[0]]]), 'r-', label='A-ENR-ELM')\n",
    "                        non_zeros_incremental_ENRELM_test = results['test_error_incremental_ENRELM'][results['test_error_incremental_ENRELM'] != 0]\n",
    "                        filled_incremental_ENRELM_test = np.ones(results['test_error_ELM'].shape[0] +1 - non_zeros_incremental_ENRELM_test.shape[0]) * non_zeros_incremental_ENRELM_test[-1]\n",
    "                        ax2.plot(X_axis[0:non_zeros_incremental_ENRELM_test.shape[0]], non_zeros_incremental_ENRELM_test, 'g-', label = \"I-ENR-ELM\")\n",
    "                        ax2.plot(X_axis[non_zeros_incremental_ENRELM_test.shape[0]:], filled_incremental_ENRELM_test, 'g--')\n",
    "                        ax2.set_ylim(ax2.get_ylim()[0],ax2.get_ylim()[1])\n",
    "\n",
    "\n",
    "\n",
    "                        ax.set_title(name)\n",
    "                        ax.set_ylabel('RMSE')\n",
    "                        ax.grid(True)\n",
    "\n",
    "                        ax2.set_title(name)\n",
    "                        ax2.set_ylabel('RMSE')\n",
    "                        ax2.grid(True)\n",
    "                    \n",
    "\n",
    "\n",
    "    folder_path = \"results/images\"\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=2, fontsize=12)\n",
    "    handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "    fig2.legend(handles2, labels2, loc='lower center', ncol=2, fontsize=12)\n",
    "    fig.subplots_adjust(bottom=0.1)\n",
    "    #fig.savefig(os.path.join(folder_path, \"image_training_synthetic_datasets_\" + str(n0) + \".eps\"), format=\"eps\")\n",
    "    fig.savefig(os.path.join(folder_path, \"image_training_synthetic_datasets_\" + str(T) + \"_\" + str(n0) + \".png\"))\n",
    "\n",
    "    fig2.subplots_adjust(bottom=0.1)\n",
    "    #fig2.savefig(os.path.join(folder_path, \"image2_test_synthetic_datasets_\" + str(n0) + \".eps\"), format=\"eps\")\n",
    "    fig2.savefig(os.path.join(folder_path, \"image2_test_synthetic_datasets_\" + str(T) + \"_\" + str(n0) + \".png\"))\n",
    "    fig.show()\n",
    "    fig2.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_synthetic(first_idx = 1, last_idx = 12, T = 300, n0 = 20)\n",
    "plot_synthetic(first_idx = 13, last_idx = 24, T = 300, n0 = 80)\n",
    "plot_synthetic(first_idx = 25, last_idx = 36, T = 1200, n0 = 20)\n",
    "plot_synthetic(first_idx = 37, last_idx = 48, T = 1200, n0 = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              dataset test_error_approximated_ENRELM_n  \\\n",
      "1             Abalone                               51   \n",
      "2            Auto MPG                               55   \n",
      "3                Bank                             1049   \n",
      "4  California Housing                             5662   \n",
      "5      Delta Ailerons                              892   \n",
      "6            LA Ozone                                4   \n",
      "7         Machine CPU                               25   \n",
      "8     Prostate Cancer                               24   \n",
      "9               Servo                              105   \n",
      "\n",
      "  test_error_approximated_ENRELM_value test_error_incremental_ENRELM_n  \\\n",
      "1                               0.6837                               7   \n",
      "2                               0.5119                              16   \n",
      "3                               0.2536                               8   \n",
      "4                               0.6078                               7   \n",
      "5                               0.5608                               5   \n",
      "6                               0.5231                               5   \n",
      "7                               0.3751                               2   \n",
      "8                               0.6465                               6   \n",
      "9                               0.4722                               7   \n",
      "\n",
      "  test_error_incremental_ENRELM_value test_error_ELM_n test_error_ELM_value  \n",
      "1                              0.6888               32    0.6721 +/- 0.0045  \n",
      "2                              0.3860               23    0.3769 +/- 0.0105  \n",
      "3                              0.2574              152    0.2154 +/- 0.0010  \n",
      "4                              0.6160              160    0.5646 +/- 0.0037  \n",
      "5                              0.5622               71    0.5517 +/- 0.0046  \n",
      "6                              0.5269                8    0.5377 +/- 0.0135  \n",
      "7                              0.3543               22    0.3871 +/- 0.1136  \n",
      "8                              0.6530                7    0.6516 +/- 0.0264  \n",
      "9                              0.4671               18    0.4940 +/- 0.0131  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = {\n",
    "    1: \"Abalone\",\n",
    "    2: \"Auto MPG\",\n",
    "    3: \"Bank\",\n",
    "    4: \"California Housing\",\n",
    "    5: \"Delta Ailerons\",\n",
    "    6: \"LA Ozone\",\n",
    "    7: \"Machine CPU\",\n",
    "    8: \"Prostate Cancer\",\n",
    "    9: \"Servo\"\n",
    "}\n",
    "\n",
    "# Columns we want to populate in the DataFrame\n",
    "metrics = [\n",
    "    'test_error_approximated_ENRELM', \n",
    "    'test_error_incremental_ENRELM', \n",
    "    'test_error_ELM'\n",
    "]\n",
    "\n",
    "# Generate new column names for each metric\n",
    "columns = ['dataset']\n",
    "for metric in metrics:\n",
    "    columns.append(f\"{metric}_n\")\n",
    "    columns.append(f\"{metric}_value\")\n",
    "\n",
    "# Initialize an empty DataFrame to store results\n",
    "df = pd.DataFrame(index=datasets.keys(), columns=columns)\n",
    "\n",
    "# Iterate through each dataset\n",
    "for key, value in datasets.items():\n",
    "    # Extract the dataset name\n",
    "    name = value\n",
    "    \n",
    "    # Load the results for the dataset\n",
    "    folder_path = 'results/datasets'\n",
    "    full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "    results = np.load(full_path)\n",
    "    df.loc[key, 'dataset'] = name\n",
    "    \n",
    "    # Populate the DataFrame for each metric\n",
    "    for metric in metrics:\n",
    "        # Extract the array for the current metric\n",
    "        metric_values = results[metric]\n",
    "        \n",
    "        # Handle ELM metrics differently as they involve mean and std\n",
    "        if \"training_error_ELM\" in metric or \"test_error_ELM\" in metric:\n",
    "            std_key = \"std_\" + metric\n",
    "            std_values = results[std_key]\n",
    "            min_index = np.argmin(metric_values)\n",
    "            min_value = metric_values[min_index]\n",
    "            std_value = std_values[min_index]\n",
    "            # Set values for the new columns\n",
    "            df.loc[key, f\"{metric}_n\"] = min_index\n",
    "            df.loc[key, f\"{metric}_value\"] = f\"{min_value:.4f} +/- {std_value:.4f}\"\n",
    "        else:\n",
    "            # For other metrics, simply find the argmin and min\n",
    "            min_index = np.argmin(metric_values)\n",
    "            min_value = metric_values[min_index]\n",
    "            # Set values for the new columns\n",
    "            df.loc[key, f\"{metric}_n\"] = min_index\n",
    "            df.loc[key, f\"{metric}_value\"] = f\"{min_value:.4f}\"\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv(os.path.join(folder_path, \"results_table_real.csv\"), index=False)\n",
    "\n",
    "# Print the DataFrame to check the output\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_table_synthetic(first_idx, last_idx):\n",
    "    # Columns we want to populate in the DataFrame\n",
    "    metrics = [\n",
    "        'test_error_approximated_ENRELM', \n",
    "        'test_error_incremental_ENRELM', \n",
    "        'test_error_ELM'\n",
    "    ]\n",
    "\n",
    "    # Generate new column names for each metric\n",
    "    columns = ['dataset']\n",
    "    for metric in metrics:\n",
    "        columns.append(f\"{metric}_n\")\n",
    "        columns.append(f\"{metric}_value\")\n",
    "\n",
    "    dataset_names = []\n",
    "    for idx in range(first_idx, last_idx+1):\n",
    "        # Extract the dataset name\n",
    "        name = \"dataset_\" + str(idx)\n",
    "        dataset_names.append(name)\n",
    "\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    df = pd.DataFrame(index=dataset_names, columns=columns)\n",
    "\n",
    "    # Iterate through each dataset\n",
    "    for name in dataset_names:\n",
    "        # Load the results for the dataset\n",
    "        folder_path = 'results/datasets'\n",
    "        full_path = os.path.join(folder_path, name + \"_results.npz\")\n",
    "        results = np.load(full_path)\n",
    "        df.loc[name, 'dataset'] = name\n",
    "        \n",
    "        # Populate the DataFrame for each metric\n",
    "        for metric in metrics:\n",
    "            # Extract the array for the current metric\n",
    "            metric_values = results[metric]\n",
    "            \n",
    "            # Handle ELM metrics differently as they involve mean and std\n",
    "            if \"training_error_ELM\" in metric or \"test_error_ELM\" in metric:\n",
    "                std_key = \"std_\" + metric\n",
    "                std_values = results[std_key]\n",
    "                min_index = np.argmin(metric_values)\n",
    "                min_value = metric_values[min_index]\n",
    "                std_value = std_values[min_index]\n",
    "                # Set values for the new columns\n",
    "                df.loc[name, f\"{metric}_n\"] = min_index\n",
    "                df.loc[name, f\"{metric}_value\"] = f\"{min_value:.4f} +/- {std_value:.4f}\"\n",
    "            else:\n",
    "                # For other metrics, simply find the argmin and min\n",
    "                min_index = np.argmin(metric_values)\n",
    "                min_value = metric_values[min_index]\n",
    "                # Set values for the new columns\n",
    "                df.loc[name, f\"{metric}_n\"] = min_index\n",
    "                df.loc[name, f\"{metric}_value\"] = f\"{min_value:.4f}\"\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(os.path.join(folder_path, \"results_table_synthetic\" + str(first_idx) + \"_\" + str(last_idx)+ \".csv\"), index=False)\n",
    "\n",
    "    # Print the DataFrame to check the output\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               dataset test_error_approximated_ENRELM_n  \\\n",
      "dataset_1    dataset_1                              200   \n",
      "dataset_2    dataset_2                               27   \n",
      "dataset_3    dataset_3                               10   \n",
      "dataset_4    dataset_4                              199   \n",
      "dataset_5    dataset_5                              205   \n",
      "dataset_6    dataset_6                              143   \n",
      "dataset_7    dataset_7                               39   \n",
      "dataset_8    dataset_8                               24   \n",
      "dataset_9    dataset_9                               18   \n",
      "dataset_10  dataset_10                               13   \n",
      "dataset_11  dataset_11                                8   \n",
      "dataset_12  dataset_12                               44   \n",
      "\n",
      "           test_error_approximated_ENRELM_value  \\\n",
      "dataset_1                                0.6536   \n",
      "dataset_2                                0.2140   \n",
      "dataset_3                                0.6429   \n",
      "dataset_4                                0.3459   \n",
      "dataset_5                                0.6078   \n",
      "dataset_6                                0.3703   \n",
      "dataset_7                                0.6546   \n",
      "dataset_8                                0.4790   \n",
      "dataset_9                                0.5363   \n",
      "dataset_10                               0.3206   \n",
      "dataset_11                               0.6634   \n",
      "dataset_12                               0.3587   \n",
      "\n",
      "           test_error_incremental_ENRELM_n  \\\n",
      "dataset_1                               20   \n",
      "dataset_2                               18   \n",
      "dataset_3                               10   \n",
      "dataset_4                               18   \n",
      "dataset_5                               17   \n",
      "dataset_6                               19   \n",
      "dataset_7                               17   \n",
      "dataset_8                               14   \n",
      "dataset_9                               10   \n",
      "dataset_10                              14   \n",
      "dataset_11                              13   \n",
      "dataset_12                              20   \n",
      "\n",
      "           test_error_incremental_ENRELM_value test_error_ELM_n  \\\n",
      "dataset_1                               0.6503               23   \n",
      "dataset_2                               0.2178               25   \n",
      "dataset_3                               0.6447               23   \n",
      "dataset_4                               0.3460               22   \n",
      "dataset_5                               0.6115               22   \n",
      "dataset_6                               0.3691               29   \n",
      "dataset_7                               0.6589               24   \n",
      "dataset_8                               0.4809               28   \n",
      "dataset_9                               0.5440               24   \n",
      "dataset_10                              0.3169               25   \n",
      "dataset_11                              0.6704               24   \n",
      "dataset_12                              0.3601               27   \n",
      "\n",
      "           test_error_ELM_value  \n",
      "dataset_1     0.6588 +/- 0.0119  \n",
      "dataset_2     0.2282 +/- 0.0111  \n",
      "dataset_3     0.7113 +/- 0.0187  \n",
      "dataset_4     0.3536 +/- 0.0106  \n",
      "dataset_5     0.6152 +/- 0.0149  \n",
      "dataset_6     0.3752 +/- 0.0110  \n",
      "dataset_7     0.6610 +/- 0.0117  \n",
      "dataset_8     0.5024 +/- 0.0140  \n",
      "dataset_9     0.5790 +/- 0.0135  \n",
      "dataset_10    0.3601 +/- 0.0085  \n",
      "dataset_11    0.6749 +/- 0.0210  \n",
      "dataset_12    0.3613 +/- 0.0088  \n",
      "               dataset test_error_approximated_ENRELM_n  \\\n",
      "dataset_13  dataset_13                               56   \n",
      "dataset_14  dataset_14                               67   \n",
      "dataset_15  dataset_15                              200   \n",
      "dataset_16  dataset_16                              106   \n",
      "dataset_17  dataset_17                               82   \n",
      "dataset_18  dataset_18                              200   \n",
      "dataset_19  dataset_19                              139   \n",
      "dataset_20  dataset_20                              121   \n",
      "dataset_21  dataset_21                              113   \n",
      "dataset_22  dataset_22                               64   \n",
      "dataset_23  dataset_23                               51   \n",
      "dataset_24  dataset_24                               41   \n",
      "\n",
      "           test_error_approximated_ENRELM_value  \\\n",
      "dataset_13                               0.6320   \n",
      "dataset_14                               0.3217   \n",
      "dataset_15                               0.6672   \n",
      "dataset_16                               0.3644   \n",
      "dataset_17                               0.6992   \n",
      "dataset_18                               0.2971   \n",
      "dataset_19                               0.7753   \n",
      "dataset_20                               0.6525   \n",
      "dataset_21                               0.8003   \n",
      "dataset_22                               0.4270   \n",
      "dataset_23                               0.5690   \n",
      "dataset_24                               0.4121   \n",
      "\n",
      "           test_error_incremental_ENRELM_n  \\\n",
      "dataset_13                              50   \n",
      "dataset_14                              59   \n",
      "dataset_15                              61   \n",
      "dataset_16                              65   \n",
      "dataset_17                              46   \n",
      "dataset_18                              70   \n",
      "dataset_19                              56   \n",
      "dataset_20                              65   \n",
      "dataset_21                              56   \n",
      "dataset_22                              57   \n",
      "dataset_23                              23   \n",
      "dataset_24                              61   \n",
      "\n",
      "           test_error_incremental_ENRELM_value test_error_ELM_n  \\\n",
      "dataset_13                              0.6256               76   \n",
      "dataset_14                              0.3313               82   \n",
      "dataset_15                              0.6640               80   \n",
      "dataset_16                              0.3591               83   \n",
      "dataset_17                              0.6918               79   \n",
      "dataset_18                              0.2999               88   \n",
      "dataset_19                              0.7738               79   \n",
      "dataset_20                              0.6476               81   \n",
      "dataset_21                              0.7796               79   \n",
      "dataset_22                              0.4099               86   \n",
      "dataset_23                              0.5712               54   \n",
      "dataset_24                              0.4020               81   \n",
      "\n",
      "           test_error_ELM_value  \n",
      "dataset_13    0.6643 +/- 0.0283  \n",
      "dataset_14    0.3370 +/- 0.0079  \n",
      "dataset_15    0.6754 +/- 0.0168  \n",
      "dataset_16    0.3839 +/- 0.0087  \n",
      "dataset_17    0.7191 +/- 0.0154  \n",
      "dataset_18    0.3118 +/- 0.0094  \n",
      "dataset_19    0.7984 +/- 0.0162  \n",
      "dataset_20    0.6816 +/- 0.0125  \n",
      "dataset_21    0.8394 +/- 0.0213  \n",
      "dataset_22    0.4527 +/- 0.0098  \n",
      "dataset_23    0.6208 +/- 0.0292  \n",
      "dataset_24    0.4195 +/- 0.0100  \n",
      "               dataset test_error_approximated_ENRELM_n  \\\n",
      "dataset_25  dataset_25                               63   \n",
      "dataset_26  dataset_26                               15   \n",
      "dataset_27  dataset_27                               18   \n",
      "dataset_28  dataset_28                              627   \n",
      "dataset_29  dataset_29                               50   \n",
      "dataset_30  dataset_30                              891   \n",
      "dataset_31  dataset_31                               10   \n",
      "dataset_32  dataset_32                               14   \n",
      "dataset_33  dataset_33                              291   \n",
      "dataset_34  dataset_34                              665   \n",
      "dataset_35  dataset_35                              183   \n",
      "dataset_36  dataset_36                              208   \n",
      "\n",
      "           test_error_approximated_ENRELM_value  \\\n",
      "dataset_25                               0.5489   \n",
      "dataset_26                               0.2981   \n",
      "dataset_27                               0.6029   \n",
      "dataset_28                               0.2991   \n",
      "dataset_29                               0.6263   \n",
      "dataset_30                               0.3042   \n",
      "dataset_31                               0.6722   \n",
      "dataset_32                               0.4102   \n",
      "dataset_33                               0.5621   \n",
      "dataset_34                               0.3312   \n",
      "dataset_35                               0.6132   \n",
      "dataset_36                               0.3291   \n",
      "\n",
      "           test_error_incremental_ENRELM_n  \\\n",
      "dataset_25                              18   \n",
      "dataset_26                              16   \n",
      "dataset_27                              19   \n",
      "dataset_28                              19   \n",
      "dataset_29                              12   \n",
      "dataset_30                              19   \n",
      "dataset_31                              11   \n",
      "dataset_32                              18   \n",
      "dataset_33                              17   \n",
      "dataset_34                              17   \n",
      "dataset_35                              17   \n",
      "dataset_36                              20   \n",
      "\n",
      "           test_error_incremental_ENRELM_value test_error_ELM_n  \\\n",
      "dataset_25                              0.5492               23   \n",
      "dataset_26                              0.3010               37   \n",
      "dataset_27                              0.6060               31   \n",
      "dataset_28                              0.3023               35   \n",
      "dataset_29                              0.6271               23   \n",
      "dataset_30                              0.3114               41   \n",
      "dataset_31                              0.6737               26   \n",
      "dataset_32                              0.4131               25   \n",
      "dataset_33                              0.5719               28   \n",
      "dataset_34                              0.3320               40   \n",
      "dataset_35                              0.6148               32   \n",
      "dataset_36                              0.3318               41   \n",
      "\n",
      "           test_error_ELM_value  \n",
      "dataset_25    0.5573 +/- 0.0043  \n",
      "dataset_26    0.3019 +/- 0.0034  \n",
      "dataset_27    0.6065 +/- 0.0048  \n",
      "dataset_28    0.3023 +/- 0.0030  \n",
      "dataset_29    0.6345 +/- 0.0055  \n",
      "dataset_30    0.3072 +/- 0.0026  \n",
      "dataset_31    0.6833 +/- 0.0040  \n",
      "dataset_32    0.4151 +/- 0.0046  \n",
      "dataset_33    0.5664 +/- 0.0032  \n",
      "dataset_34    0.3325 +/- 0.0041  \n",
      "dataset_35    0.6115 +/- 0.0034  \n",
      "dataset_36    0.3222 +/- 0.0036  \n",
      "               dataset test_error_approximated_ENRELM_n  \\\n",
      "dataset_37  dataset_37                              898   \n",
      "dataset_38  dataset_38                              141   \n",
      "dataset_39  dataset_39                              219   \n",
      "dataset_40  dataset_40                              648   \n",
      "dataset_41  dataset_41                              472   \n",
      "dataset_42  dataset_42                              386   \n",
      "dataset_43  dataset_43                              586   \n",
      "dataset_44  dataset_44                              344   \n",
      "dataset_45  dataset_45                              787   \n",
      "dataset_46  dataset_46                              307   \n",
      "dataset_47  dataset_47                              254   \n",
      "dataset_48  dataset_48                              112   \n",
      "\n",
      "           test_error_approximated_ENRELM_value  \\\n",
      "dataset_37                               0.5628   \n",
      "dataset_38                               0.3228   \n",
      "dataset_39                               0.6339   \n",
      "dataset_40                               0.3171   \n",
      "dataset_41                               0.5684   \n",
      "dataset_42                               0.3416   \n",
      "dataset_43                               0.6844   \n",
      "dataset_44                               0.4807   \n",
      "dataset_45                               0.6031   \n",
      "dataset_46                               0.3804   \n",
      "dataset_47                               0.6062   \n",
      "dataset_48                               0.3320   \n",
      "\n",
      "           test_error_incremental_ENRELM_n  \\\n",
      "dataset_37                              66   \n",
      "dataset_38                              70   \n",
      "dataset_39                              68   \n",
      "dataset_40                              76   \n",
      "dataset_41                              39   \n",
      "dataset_42                              66   \n",
      "dataset_43                              65   \n",
      "dataset_44                              67   \n",
      "dataset_45                              68   \n",
      "dataset_46                              75   \n",
      "dataset_47                              65   \n",
      "dataset_48                              55   \n",
      "\n",
      "           test_error_incremental_ENRELM_value test_error_ELM_n  \\\n",
      "dataset_37                              0.5687               83   \n",
      "dataset_38                              0.3369               92   \n",
      "dataset_39                              0.6375               83   \n",
      "dataset_40                              0.3311               89   \n",
      "dataset_41                              0.5692               78   \n",
      "dataset_42                              0.3481               93   \n",
      "dataset_43                              0.6836               83   \n",
      "dataset_44                              0.4915               82   \n",
      "dataset_45                              0.6135               86   \n",
      "dataset_46                              0.3887               86   \n",
      "dataset_47                              0.5965               87   \n",
      "dataset_48                              0.3392               85   \n",
      "\n",
      "           test_error_ELM_value  \n",
      "dataset_37    0.5641 +/- 0.0036  \n",
      "dataset_38    0.3259 +/- 0.0026  \n",
      "dataset_39    0.6404 +/- 0.0031  \n",
      "dataset_40    0.3195 +/- 0.0027  \n",
      "dataset_41    0.5727 +/- 0.0036  \n",
      "dataset_42    0.3457 +/- 0.0027  \n",
      "dataset_43    0.6866 +/- 0.0025  \n",
      "dataset_44    0.4835 +/- 0.0038  \n",
      "dataset_45    0.6058 +/- 0.0038  \n",
      "dataset_46    0.3829 +/- 0.0035  \n",
      "dataset_47    0.6133 +/- 0.0049  \n",
      "dataset_48    0.3373 +/- 0.0032  \n"
     ]
    }
   ],
   "source": [
    "performance_table_synthetic(first_idx = 1, last_idx = 12)\n",
    "performance_table_synthetic(first_idx = 13, last_idx = 24)\n",
    "performance_table_synthetic(first_idx = 25, last_idx = 36)\n",
    "performance_table_synthetic(first_idx = 37, last_idx = 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              dataset  timing_approximated_ENRELM  timing_incremental_ENRELM  \\\n",
      "0             Abalone                 10343.53460               1.574766e+04   \n",
      "1            Auto MPG                    26.97792               1.977972e+02   \n",
      "2                Bank                 64383.83666               2.957264e+05   \n",
      "3  California Housing                968040.58580               2.848915e+06   \n",
      "4      Delta Ailerons                 43306.73760               2.011230e+05   \n",
      "5            LA Ozone                    19.48166               6.212156e+01   \n",
      "6         Machine CPU                     6.73802               4.787738e+01   \n",
      "7     Prostate Cancer                     3.28962               1.298700e+01   \n",
      "8               Servo                     6.23774               8.033042e+01   \n",
      "\n",
      "   total_timing_ELM  mean_loop_timing_ELM  \n",
      "0      6.449388e+05          32246.938841  \n",
      "1      9.929391e+03            496.469531  \n",
      "2      7.359793e+05          36798.963348  \n",
      "3      1.751046e+06          87552.281978  \n",
      "4      2.591535e+05          12957.673831  \n",
      "5      7.074074e+03            353.703679  \n",
      "6      2.223186e+03            111.159289  \n",
      "7      2.339594e+02             11.697971  \n",
      "8      1.163113e+03             58.155652  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the compressed file containing the dataframe\n",
    "folder_path = 'results/datasets'\n",
    "filename = \"times_real.csv\"\n",
    "full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "\n",
    "# Extracting the dataframe from the loaded data\n",
    "df_times = pd.read_csv(full_path, sep=\";\").drop(columns=[\"iteration\"])\n",
    "\n",
    "# Grouping by 'dataset' and calculating the mean for all other columns\n",
    "df_means = df_times.groupby(\"dataset\", as_index=False).mean()\n",
    "\n",
    "# The 'iteration' column is removed from df_means automatically because it's not used in the averaging process.\n",
    "df_means.to_csv(os.path.join(folder_path, \"times_table_real.csv\"), index=False)\n",
    "print(df_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dataset  timing_approximated_ENRELM  timing_incremental_ENRELM  \\\n",
      "0    dataset_1                    16.32146                  124.96869   \n",
      "1    dataset_2                    14.18106                  140.60400   \n",
      "2    dataset_3                    14.54270                  117.02895   \n",
      "3    dataset_4                    14.25233                  121.38743   \n",
      "4    dataset_5                    14.08346                  105.09719   \n",
      "5    dataset_6                    14.75797                  107.49984   \n",
      "6    dataset_7                    15.93076                  115.34181   \n",
      "7    dataset_8                    13.86249                  114.22304   \n",
      "8    dataset_9                    14.53062                   94.65098   \n",
      "9   dataset_10                    13.96850                  143.14260   \n",
      "10  dataset_11                    14.24737                  106.52110   \n",
      "11  dataset_12                    14.36173                  136.39106   \n",
      "12  dataset_13                    20.54475                  333.03524   \n",
      "13  dataset_14                    24.03600                  435.90489   \n",
      "14  dataset_15                    16.05688                  335.69949   \n",
      "15  dataset_16                    16.03769                  408.20349   \n",
      "16  dataset_17                    15.10580                  278.86309   \n",
      "17  dataset_18                    18.08123                  403.01774   \n",
      "18  dataset_19                    15.46903                  312.33403   \n",
      "19  dataset_20                    15.08143                  369.08719   \n",
      "20  dataset_21                    17.08805                  329.02546   \n",
      "21  dataset_22                    16.03508                  370.74961   \n",
      "22  dataset_23                    15.17585                  262.05518   \n",
      "23  dataset_24                    15.59896                  300.90870   \n",
      "24  dataset_25                   338.16093                  687.32146   \n",
      "25  dataset_26                   325.93514                  748.87876   \n",
      "26  dataset_27                   335.48370                  764.41084   \n",
      "27  dataset_28                   329.64922                  715.07219   \n",
      "28  dataset_29                   321.74117                  595.92242   \n",
      "29  dataset_30                   326.42065                  730.79328   \n",
      "30  dataset_31                   331.72239                  666.00263   \n",
      "31  dataset_32                   321.78727                  692.16751   \n",
      "32  dataset_33                   329.06390                  717.21019   \n",
      "33  dataset_34                   328.76441                  697.19879   \n",
      "34  dataset_35                   330.05414                  579.63691   \n",
      "35  dataset_36                   325.93300                  694.37587   \n",
      "36  dataset_37                   357.21692                 1448.53690   \n",
      "37  dataset_38                   345.47655                 1598.43018   \n",
      "38  dataset_39                   344.23160                 1365.46915   \n",
      "39  dataset_40                   349.53607                 1652.66787   \n",
      "40  dataset_41                   342.46273                 1050.00356   \n",
      "41  dataset_42                   344.69478                 1194.74726   \n",
      "42  dataset_43                   347.03367                 1358.38221   \n",
      "43  dataset_44                   350.87913                 1350.14249   \n",
      "44  dataset_45                   346.11636                 1254.28499   \n",
      "45  dataset_46                   358.33137                 1608.32152   \n",
      "46  dataset_47                   342.84621                 1152.46139   \n",
      "47  dataset_48                   353.20407                 1135.06300   \n",
      "\n",
      "    total_timing_ELM  mean_loop_timing_ELM  \n",
      "0         5307.37014            265.368507  \n",
      "1         3787.67999            189.383999  \n",
      "2         3738.63963            186.931981  \n",
      "3         4370.76972            218.538486  \n",
      "4         3819.09180            190.954590  \n",
      "5         3645.48390            182.274195  \n",
      "6         3760.09064            188.004532  \n",
      "7         3777.84259            188.892129  \n",
      "8         3748.16894            187.408447  \n",
      "9         3757.50901            187.875450  \n",
      "10        6491.89338            324.594669  \n",
      "11        5169.10815            258.455407  \n",
      "12        6069.08772            303.454386  \n",
      "13        5367.00658            268.350329  \n",
      "14        4970.70184            248.535092  \n",
      "15        4678.19782            233.909891  \n",
      "16        4474.52763            223.726381  \n",
      "17        4430.76436            221.538218  \n",
      "18        4566.51599            228.325799  \n",
      "19        4375.69521            218.784760  \n",
      "20        4371.82896            218.591448  \n",
      "21        4475.03478            223.751739  \n",
      "22        4395.86727            219.793363  \n",
      "23        4304.27108            215.213554  \n",
      "24      135352.25036           6767.612518  \n",
      "25      130395.48539           6519.774269  \n",
      "26      130706.77772           6535.338886  \n",
      "27      131380.34171           6569.017086  \n",
      "28      132626.76983           6631.338491  \n",
      "29      128876.64502           6443.832251  \n",
      "30      129583.91208           6479.195604  \n",
      "31      130419.67952           6520.983976  \n",
      "32      130369.22295           6518.461148  \n",
      "33      131257.85343           6562.892672  \n",
      "34      132284.08713           6614.204356  \n",
      "35      150845.70616           7542.285308  \n",
      "36      140409.61809           7020.480905  \n",
      "37      138844.50821           6942.225410  \n",
      "38      140113.51526           7005.675763  \n",
      "39      142978.33630           7148.916815  \n",
      "40      138376.15035           6918.807517  \n",
      "41      139728.52306           6986.426153  \n",
      "42      137371.92784           6868.596392  \n",
      "43      141273.82691           7063.691346  \n",
      "44      141724.36133           7086.218067  \n",
      "45      138566.31555           6928.315777  \n",
      "46      140474.07547           7023.703774  \n",
      "47      139754.34675           6987.717337  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the compressed file containing the dataframe\n",
    "folder_path = 'results/datasets'\n",
    "first_idx = 1\n",
    "last_idx = 48\n",
    "filename = \"times_synthetic\"+ str(first_idx) + \"_\" + str(last_idx)+\".csv\"\n",
    "full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "\n",
    "# Extracting the dataframe from the loaded data\n",
    "df_times = pd.read_csv(full_path, sep=\";\").drop(columns=[\"iteration\"])\n",
    "\n",
    "# Grouping by 'dataset' and calculating the mean for all other columns\n",
    "df_means = df_times.groupby(\"dataset\", as_index=False).mean()\n",
    "\n",
    "# Extract the number from each dataset and convert it to an integer\n",
    "df_means['number'] = df_means['dataset'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Sort the dataframe based on the extracted number\n",
    "df_sorted = df_means.sort_values('number').drop('number', axis=1)\n",
    "\n",
    "# Reset index\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "# The 'iteration' column is removed from df_sorted automatically because it's not used in the averaging process.\n",
    "df_sorted.to_csv(os.path.join(folder_path, \"times_table_synthetic\" + str(first_idx) + \"_\" + str(last_idx)+ \".csv\"), index=False)\n",
    "print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
